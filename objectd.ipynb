{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\suyog\\Downloads\\custom_data'\n",
    "image_size = (224, 224)  # Adjust to your preferred image size\n",
    "batch_size = 32\n",
    "num_classes = 2  # Specify the number of classes in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_gen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 images belonging to 1 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "validation_generator = data_gen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Build and compile the model (MobileNetV2 with fine-tuning)\n",
    "base_model = keras.applications.MobileNetV2(input_shape=image_size + (3,), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Freeze the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 - 32s - loss: 1.8488 - accuracy: 0.5250 - val_loss: 1.4934 - val_accuracy: 0.5250 - 32s/epoch - 3s/step\n",
      "Epoch 2/10\n",
      "10/10 - 16s - loss: 1.7922 - accuracy: 0.5437 - val_loss: 1.5054 - val_accuracy: 0.5500 - 16s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "10/10 - 32s - loss: 2.0301 - accuracy: 0.4406 - val_loss: 1.5183 - val_accuracy: 0.5000 - 32s/epoch - 3s/step\n",
      "Epoch 4/10\n",
      "10/10 - 22s - loss: 2.2431 - accuracy: 0.4531 - val_loss: 1.5095 - val_accuracy: 0.4875 - 22s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "10/10 - 24s - loss: 2.6391 - accuracy: 0.4750 - val_loss: 1.5493 - val_accuracy: 0.6125 - 24s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "10/10 - 13s - loss: 2.9521 - accuracy: 0.4563 - val_loss: 1.5210 - val_accuracy: 0.4750 - 13s/epoch - 1s/step\n",
      "Epoch 7/10\n",
      "10/10 - 13s - loss: 3.2580 - accuracy: 0.4875 - val_loss: 1.5246 - val_accuracy: 0.3375 - 13s/epoch - 1s/step\n",
      "Epoch 8/10\n",
      "10/10 - 13s - loss: 3.6753 - accuracy: 0.5031 - val_loss: 1.6448 - val_accuracy: 0.2000 - 13s/epoch - 1s/step\n",
      "Epoch 9/10\n",
      "10/10 - 13s - loss: 4.2463 - accuracy: 0.4719 - val_loss: 1.5897 - val_accuracy: 0.2000 - 13s/epoch - 1s/step\n",
      "Epoch 10/10\n",
      "10/10 - 13s - loss: 4.9484 - accuracy: 0.4469 - val_loss: 1.6262 - val_accuracy: 0.1750 - 13s/epoch - 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 10  # Adjust the number of epochs as needed\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('custom_object_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model(\"custom_object_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"knife\", \"guns\", \"others\"]\n",
    "\n",
    "# Load the custom object classification model\n",
    "model = tf.keras.models.load_model(\"custom_object_classifier.h5\")\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_frame(frame, input_size=(224, 224)):\n",
    "    # Resize the frame to the specified input size\n",
    "    frame = cv2.resize(frame, input_size)\n",
    "\n",
    "    # Normalize pixel values to be in the range [0, 1]\n",
    "    frame = frame / 255.0\n",
    "\n",
    "    # Expand dimensions to create a batch of size 1\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(0)  # To open the default camera (0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    processed_frame = preprocess_frame(frame)\n",
    "\n",
    "    # Perform object classification\n",
    "    predictions = model.predict(processed_frame)\n",
    "    class_id = np.argmax(predictions[0])\n",
    "    class_name = class_names[class_id]\n",
    "\n",
    "    # Display the classification result on the frame\n",
    "    cv2.putText(frame, f\"Class: {class_name}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the processed frame\n",
    "    cv2.imshow(\"Object Classification\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv3 model and classes\n",
    "net = cv2.dnn.readNet(r\"C:\\Users\\suyog\\violence_detection_app\\yolov3.weights\", r\"C:\\Users\\suyog\\violence_detection_app\\yolov3.cfg\")\n",
    "\n",
    "with open(r\"C:\\Users\\suyog\\violence_detection_app\\coco.names\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(0)  # To open the default camera (0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get the height and width of the frame\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Preprocess the frame for YOLOv3\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "    # Lists to store detected objects' information\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # Process the detected objects\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:  # Adjust the confidence threshold as needed\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maximum suppression to filter out overlapping objects\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Draw bounding boxes and labels on the frame\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)  # BGR color for the bounding box (green in this case)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Display the frame with object detection\n",
    "    cv2.imshow(\"YOLOv3 Object Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
